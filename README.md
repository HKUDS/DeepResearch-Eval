<div align="center">

# ğŸ“Š Understanding DeepResearch via Reports

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
<a href='https://arxiv.org/abs/2510.07861'><img src='https://img.shields.io/badge/arXiv-2510.07861-b31b1b'>

**ğŸ” A toolkit for automated evaluation and fact-checking of long technical/research reports**

*Focus on report quality assessment and factuality verification*

</div>

---
## Why understand a deep research system via its reports?

Reports are the most canonical and representative outputs of DeepResearch. High-quality research reports feature clear structure, rigorous logic, dense information, and trustworthy citationsâ€”crucial for knowledge-intensive research scenarios. To this end, we propose the DeepResearch-ReportEval framework: a hybrid evaluation approach combining LLM-as-a-Judge for automated report-quality assessment with expert judgments for reliability. The framework evaluates report quality across multiple dimensions, including comprehensiveness, redundancy, and factual accuracy. We also release a carefully curated dataset: 100 queries spanning diverse categories and 100 corresponding reports generated by [Qwen-DeepResearch](https://chat.qwen.ai/?inputFeature=deep_research) to support systematic evaluation.


## âœ¨ Highlights

<div align="center">

| ğŸ¯ Quality Scoring | ğŸ” Fact Checking | ğŸ“ˆ Redundancy Detection |
|:---:|:---:|:---:|
| Five-dimension scoring | Web content verification | Pairwise paragraph analysis |

</div>

## ğŸš€ Core Features

### ğŸ“Š Quality Evaluation (`judge_score.py`)
- âœ… **Five dimensions**: Comprehensiveness, Coherence, Clarity, Insightfulness, Overall
- ğŸ”„ **Redundancy detection**: Smart sampling of paragraph pairs to compute average redundancy score
- ğŸ“ **Rationales**: Provide reasoned explanations for the scores
- ğŸ’¾ **Checkpointing**: Resume from checkpoints to avoid recomputation

### ğŸ” Fact Checking (`judge_fact.py`)
- ğŸŒ **Web scraping**: Support both Firecrawl and Jina Reader
- ğŸ“‹ **Batch verification**: Check each provided context against the scraped page
- ğŸ¯ **Ternary scoring**: -1 (Not supported) / 0 (Uncertain) / 1 (Supported)
- ğŸ“„ **Explanations**: Provide supporting evidence and analysis

---

## ğŸ“ Project Structure

```
DeepResearch-ReportEval/
â”œâ”€â”€ ğŸ“Š judge_score.py          # Main script for quality evaluation
â”œâ”€â”€ ğŸ” judge_fact.py           # Main script for fact checking
â”œâ”€â”€ ğŸ› ï¸ Atools.py               # Utilities and model calls
â”œâ”€â”€ ğŸ“ Aprompts.py             # Prompt templates
â”œâ”€â”€ ğŸ“‚ data/                   # Dataset
â”‚   â”œâ”€â”€ topic/                 # High-quality topics
â”‚   â””â”€â”€ report/                # Reports from Qwen-DeepResearch, collected in early September, 2025
â””â”€â”€ ğŸ“‹ example/                # Example inputs & outputs
    â”œâ”€â”€ judge_fact_result/     # Fact-checking examples
    â””â”€â”€ judge_score_result/    # Quality and Redundancy examples
```

---

## âš™ï¸ Environment Setup

### ğŸ“‹ Requirements
- **Python**: 3.10+
- **OS**: Windows / macOS / Linux

### ğŸ“¦ Installation

```bash
# Clone the project
git clone https://github.com/HKUDS/DeepResearch-Eval.git
cd DeepResearch-Eval

# Install dependencies
pip install openai json-repair firecrawl-python python-dotenv tqdm requests dashscope
```

### ğŸ”‘ Environment Variables

Create a `.env` file or export env vars:

```bash
# Required
export OPENAI_API_KEY="your-openai-api-key"
export FIRECRAWL_KEY="your-firecrawl-key"        # or
export JINA_API_KEY="your-jina-api-key"

# Optional
export OPENAI_API_BASE="your api base"  # Custom API endpoint
```

---

## ğŸ“Š Data Formats

### ğŸ“ Quality Evaluation

**Input** (JSONL):
```json
{"topic": "Applications of AI in Healthcare", "report": "# Report Title\n\n## Introduction\n..."}
```

**Output** (JSON):
```json
{
  "file_id": "abc123...",
  "topic": "Applications of AI in Healthcare",
  "comprehensiveness_score": 2,
  "coherence_score": 3,
  "clarity_score": 4,
  "insight_score": 3,
  "overall_score": 3,
  "quality_reason": "The report is well-structured with sufficient arguments...",
  "repeat_score": 3.12,
  "repeat_results": [...]
}
```

### ğŸ” Fact Checking

**Input** (JSONL):
```json
{"https://example.com/page": {"contexts": ["Sentence A", "Sentence B", ...]}}
```

**Output** (JSONL):
```json
{"url": "https://example.com/page", "context": "Sentence A", "label": {"is_factual": 1, "sentence_support": "..."}}
```

> ğŸ“Œ **Scoring**: `is_factual` âˆˆ { -1 (Not supported), 0 (Uncertain), 1 (Supported) }

---

## ğŸš€ Quick Start

### ğŸ“‹ Prepare Data
- **Quality evaluation**: Use `data/topic/high_quality_topics.jsonl` or your own JSONL
- **Fact checking**: Refer to `example/judge_fact_result/example_fact_judge_input.jsonl`

### ğŸ’» Examples

#### ğŸ“Š Quality Evaluation
```bash
# Basic run
python judge_score.py \
  --inputpath data/topic/high_quality_topics.jsonl \
  --outputpath exp/score_results

# Resume from checkpoint
python judge_score.py \
  --inputpath data/topic/high_quality_topics.jsonl \
  --outputpath exp/score_results \
  --resume

# Clear checkpoint and restart
python judge_score.py \
  --inputpath data/topic/high_quality_topics.jsonl \
  --outputpath exp/score_results \
  --clear_checkpoint
```

#### ğŸ” Fact Checking
```bash
# Judge mode (default)
python judge_fact.py \
  --inputpath example/judge_fact_result/example_fact_judge_input.jsonl \
  --outputpath example/judge_fact_result/example_fact_judge_output.jsonl \
  --provider jina \
  --limit 3 \
  --task judge

# Scrape-only mode
python judge_fact.py \
  --inputpath example/judge_fact_result/example_fact_judge_input.jsonl \
  --outputpath example/judge_fact_result/example_fact_scrape.out.jsonl \
  --provider jina \
  --limit 3 \
  --task scrape
```

---

## ğŸ“ Outputs

### ğŸ“Š Quality Evaluation Outputs
```
exp/score_results/
â”œâ”€â”€ abc123def456.json    # Evaluation result for topic 1
â”œâ”€â”€ def456ghi789.json    # Evaluation result for topic 2
â””â”€â”€ ...

exp/
â”œâ”€â”€ judge.txt            # Detailed logs
â”œâ”€â”€ judge.json           # Progress records
â””â”€â”€ checkpoint.json      # Checkpoint state
```

### ğŸ” Fact Checking Outputs
```
example/judge_fact_result/
â”œâ”€â”€ example_fact_judge_output.jsonl    # Judging results
â””â”€â”€ example_fact_scrape.out.jsonl      # Scraped content
```

---

## â“ FAQ

<details>
<summary><strong>ğŸŒ Failing to scrape pages?</strong></summary>

- âœ… Ensure `FIRECRAWL_KEY` or `JINA_API_KEY` is set
- ğŸ”„ Try switching `--provider` (firecrawl/jina)
- âš¡ Some sites may throttle/deny access; try lower concurrency

</details>

<details>
<summary><strong>ğŸ¤– Model output parsing errors?</strong></summary>

- ğŸ› ï¸ We use `json_repair` for robust parsing
- ğŸ”„ Built-in retries; failures are logged and skipped
- ğŸ“ Check `./exp/judge.txt` for detailed errors

</details>

<details>
<summary><strong>ğŸ“Š Zero redundancy pairs?</strong></summary>

- ğŸ“‹ Ensure the report contains first-level headings starting with `## `
- ğŸ“ Increase report length (recommended > 200 characters)

</details>

---

## ğŸ“„ License

**Released under the [MIT License](LICENSE)**
